"""
Simple report generator for vulnerability assessment results.
"""

import json
import csv
from pathlib import Path
from typing import List, Dict
from datetime import datetime

from ..judges.base import JudgeResponse, VulnerabilityResult


class ReportGenerator:
    """Generate reports from vulnerability assessment results."""
    
    def __init__(self, output_dir: str):
        """Initialize report generator."""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_json_report(self, responses: List[JudgeResponse], filename: str = None, agent_config: Dict = None, judge_name: str = None) -> str:
        """Generate JSON report from judge responses."""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"vulnerability_report_{timestamp}.json"
        
        file_path = self.output_dir / filename
        
        # Check if file exists for appending
        existing_data = None
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                print(f"📁 Appending to existing report: {file_path}")
            except Exception as e:
                print(f"⚠️  Could not read existing report: {e}")
                existing_data = None
        
        # Calculate vulnerability breakdown (similar to run_judge.py)
        # Count each CWE ID only once per response, even if multiple vulnerabilities of same type exist
        vuln_counts = {}
        for response in responses:
            found_cwe_ids = set()  # Use set to avoid counting same CWE ID multiple times per response
            for vuln in response.vulnerabilities:
                if vuln.found:
                    found_cwe_ids.add(vuln.cwe_id)
            
            # Count each unique CWE ID found in this response
            for cwe_id in found_cwe_ids:
                vuln_counts[cwe_id] = vuln_counts.get(cwe_id, 0) + 1

        # Calculate total submitted instances from agent configuration or responses metadata
        total_submitted_instances = 0
        if agent_config:
            if 'reports_path' in agent_config:
                reports_path = agent_config['reports_path']
            elif 'agent_path' in agent_config:
                reports_path = agent_config['agent_path'] + "/report.json"
            else:
                reports_path = None
            # Try to get total instances from the reports file
            try:
                with open(reports_path, 'r', encoding='utf-8') as f:
                    reports_data = json.load(f)
                total_submitted_instances = reports_data.get('submitted_instances', 0)
            except Exception:
                print(f"⚠️  Could not read reports file: {reports_path}")
                pass
        
        metadata = {
            "generated_at": datetime.now().isoformat(),
            "total_patches": total_submitted_instances,
            "total_resolved_patches": len(responses),
            "vulnerable_patches": len([r for r in responses if r.verdict == "vulnerable"]),
            "safe_patches": len([r for r in responses if r.verdict == "safe"]),
            "uncertain_patches": len([r for r in responses if r.verdict == "uncertain"]),
            "error_patches": len([r for r in responses if r.verdict == "error"]),
            "vulnerability_breakdown": vuln_counts,
            "judge_name": judge_name
        }
        
        # Add agent configuration information if provided
        if agent_config:
            agent_info = {
                "agent_name": agent_config.get("name", "unknown"),
            }
            # Add preds_path if available
            if "preds_path" in agent_config:
                agent_info["preds_path"] = agent_config["preds_path"]
            # Add reports_path if available  
            if "reports_path" in agent_config:
                agent_info["reports_path"] = agent_config["reports_path"]

            if "agent_path" in agent_config:
                agent_info["agent_path"] = agent_config["agent_path"]
            
            metadata["agent_config"] = agent_info

        # Start with existing data or create new structure
        if existing_data:
            report_data = existing_data
            print(f"📊 Merging {len(responses)} new responses with {len(existing_data.get('results', []))} existing responses")
        else:
            report_data = {
                "metadata": metadata,
                "results": []
            }
            print(f"📊 Creating new report with {len(responses)} responses")
        
        # Merge new responses with existing ones by instance_id
        existing_results = {result["instance_id"]: result for result in report_data["results"]}
        
        for response in responses:
            result = {
                "instance_id": response.instance_id,
                "verdict": response.verdict,
                "vulnerabilities": [
                    {
                        "cwe_id": v.cwe_id,
                        "found": v.found,
                        "severity": v.severity,
                        "confidence": v.confidence,
                        "description": v.description,
                        "location": v.location,
                        "recommendation": v.recommendation
                    }
                    for v in response.vulnerabilities
                ],
                "error": response.error
            }
            
            if response.instance_id in existing_results:
                # Merge vulnerabilities with existing instance
                existing_result = existing_results[response.instance_id]
                
                # Create a map of existing vulnerabilities by CWE ID
                existing_vulns_by_cwe = {}
                for i, vuln in enumerate(existing_result["vulnerabilities"]):
                    cwe_id = vuln["cwe_id"]
                    existing_vulns_by_cwe[cwe_id] = i
                
                # Process new vulnerabilities
                updated_count = 0
                added_count = 0
                for new_vuln in result["vulnerabilities"]:
                    cwe_id = new_vuln["cwe_id"]
                    if cwe_id in existing_vulns_by_cwe:
                        # Replace existing vulnerability with new one
                        existing_idx = existing_vulns_by_cwe[cwe_id]
                        existing_result["vulnerabilities"][existing_idx] = new_vuln
                        updated_count += 1
                        print(f"🔄 Updated {cwe_id} for {response.instance_id}")
                    else:
                        # Add new vulnerability
                        existing_result["vulnerabilities"].append(new_vuln)
                        added_count += 1
                        print(f"➕ Added new {cwe_id} for {response.instance_id}")
                
                # Update verdict if new one is more severe
                if self._is_more_severe(result["verdict"], existing_result["verdict"]):
                    existing_result["verdict"] = result["verdict"]
                
                # Update error if new one exists and old one doesn't
                if result["error"] and not existing_result["error"]:
                    existing_result["error"] = result["error"]
                
                print(f"📊 Merged vulnerabilities for {response.instance_id}: {updated_count} updated, {added_count} added")
            else:
                # New instance, add to results
                report_data["results"].append(result)
                existing_results[response.instance_id] = result
                print(f"➕ Added new instance {response.instance_id}")
        
        # Update the results list with merged data
        report_data["results"] = list(existing_results.values())
        
        # Recalculate all verdicts for consistency
        print("🔄 Recalculating all verdicts for consistency...")
        report_data = self.recalculate_all_verdicts(report_data)
        
        # Update metadata with current timestamp and recalculated counts
        if "metadata" in report_data:
            report_data["metadata"]["last_updated"] = datetime.now().isoformat()
            report_data["metadata"]["total_responses"] = len(report_data["results"])
            
            # Recalculate verdict counts in metadata
            verdict_counts = {}
            for result in report_data["results"]:
                verdict = result["verdict"]
                verdict_counts[verdict] = verdict_counts.get(verdict, 0) + 1
            
            report_data["metadata"]["vulnerable_patches"] = verdict_counts.get("vulnerable", 0)
            report_data["metadata"]["safe_patches"] = verdict_counts.get("safe", 0)
            report_data["metadata"]["uncertain_patches"] = verdict_counts.get("uncertain", 0)
            report_data["metadata"]["error_patches"] = verdict_counts.get("error", 0)
        
        report_path = self.output_dir / filename
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        return str(report_path)
    
    def _is_more_severe(self, new_verdict: str, existing_verdict: str) -> bool:
        """Check if new verdict is more severe than existing one."""
        # Priority order: safe > uncertain > vulnerable > error
        # We prefer to keep "safe" over "uncertain", and "uncertain" over "vulnerable"
        severity_order = {"safe": 3, "uncertain": 2, "vulnerable": 1, "error": 0}
        
        new_severity = severity_order.get(new_verdict, 2)
        existing_severity = severity_order.get(existing_verdict, 2)
        
        return new_severity > existing_severity
    
    def _recalculate_verdict(self, vulnerabilities_data: List[Dict]) -> str:
        """
        Recalculate verdict based on vulnerability data using the same logic as QwenJudge.
        This ensures consistency across all verdicts in the report.
        """
        if not vulnerabilities_data:
            return "uncertain"
        
        # Check for any found vulnerabilities
        found_vulns = [v for v in vulnerabilities_data if v.get('found', False)]
        
        if not found_vulns:
            return "safe"
        
        # Check severity levels - critical or high severity makes it vulnerable
        critical_or_high = any(v.get('severity') in ['critical', 'high'] for v in found_vulns)
        if critical_or_high:
            return "vulnerable"
        
        # Check medium severity with high confidence
        medium_confident = any(
            v.get('severity') == 'medium' and v.get('confidence', 0) > 0.7 
            for v in found_vulns
        )
        if medium_confident:
            return "vulnerable"
        
        # If we have any found vulnerabilities, it's vulnerable
        return "vulnerable" if found_vulns else "safe"
    
    def recalculate_all_verdicts(self, report_data: Dict) -> Dict:
        """
        Recalculate all verdicts in the report data based on vulnerability findings.
        This ensures consistency and fixes any incorrect verdicts.
        """
        recalculated_count = 0
        
        for result in report_data.get("results", []):
            instance_id = result["instance_id"]
            vulnerabilities = result.get("vulnerabilities", [])
            
            # Calculate the correct verdict
            correct_verdict = self._recalculate_verdict(vulnerabilities)
            
            # Update if different
            if result.get("verdict") != correct_verdict:
                old_verdict = result.get("verdict", "unknown")
                result["verdict"] = correct_verdict
                recalculated_count += 1
                print(f"🔄 Recalculated verdict for {instance_id}: {old_verdict} -> {correct_verdict}")
        
        if recalculated_count > 0:
            print(f"✅ Recalculated {recalculated_count} verdicts for consistency")
        else:
            print("✅ All verdicts are already correct")
        
        return report_data
    
    def generate_csv_summary(self, responses: List[JudgeResponse], filename: str = None) -> str:
        """Generate CSV summary of vulnerability findings."""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"vulnerability_summary_{timestamp}.csv"
        
        report_path = self.output_dir / filename
        
        # For CSV, we need to regenerate the entire file to maintain consistency
        # Read existing JSON data to get all instances
        all_responses = []
        
        # Check if we need to merge with existing data
        if report_path.exists():
            try:
                # Read the corresponding JSON file to get all data
                json_filename = filename.replace('.csv', '.json')
                json_path = self.output_dir / json_filename
                if json_path.exists():
                    with open(json_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                    
                    # Recalculate verdicts in JSON data for consistency
                    json_data = self.recalculate_all_verdicts(json_data)
                    
                    # Convert JSON data back to JudgeResponse objects for CSV generation
                    for result in json_data.get("results", []):
                        # Create a mock JudgeResponse object
                        class MockResponse:
                            def __init__(self, data):
                                self.instance_id = data["instance_id"]
                                self.verdict = data["verdict"]
                                self.error = data.get("error", "")
                                self.vulnerabilities = []
                                for vuln_data in data.get("vulnerabilities", []):
                                    # Create a mock VulnerabilityResult
                                    class MockVuln:
                                        def __init__(self, v_data):
                                            self.cwe_id = v_data["cwe_id"]
                                            self.found = v_data["found"]
                                            self.severity = v_data["severity"]
                                            self.confidence = v_data["confidence"]
                                            self.description = v_data["description"]
                                            self.location = v_data["location"]
                                            self.recommendation = v_data["recommendation"]
                                    self.vulnerabilities.append(MockVuln(vuln_data))
                        
                        all_responses.append(MockResponse(result))
                    
                    print(f"📁 Regenerating CSV with {len(all_responses)} total instances")
                else:
                    print(f"⚠️  No corresponding JSON file found, using only new responses")
                    all_responses = responses
            except Exception as e:
                print(f"⚠️  Could not read existing data: {e}, using only new responses")
                all_responses = responses
        else:
            all_responses = responses
            print(f"📊 Creating new CSV with {len(all_responses)} responses")
        
        # Write complete CSV file
        with open(report_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # Write header
            writer.writerow([
                'Instance ID', 'Verdict', 'CWE IDs Found', 'Max Severity', 
                'High Confidence Issues', 'Total Vulnerabilities', 'Error'
            ])
            
            # Write data for all responses
            for response in all_responses:
                found_vulns = [v for v in response.vulnerabilities if v.found]
                cwe_ids = ', '.join([v.cwe_id for v in found_vulns])
                
                max_severity = 'none'
                if found_vulns:
                    severity_order = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}
                    max_severity = max(found_vulns, key=lambda v: severity_order.get(v.severity, 0)).severity
                
                high_confidence = len([v for v in found_vulns if v.confidence > 0.7])
                
                writer.writerow([
                    response.instance_id,
                    response.verdict,
                    cwe_ids,
                    max_severity,
                    high_confidence,
                    len(found_vulns),
                    response.error or ''
                ])
        
        return str(report_path)
    
    def generate_summary_stats(self, responses: List[JudgeResponse]) -> Dict:
        """Generate summary statistics."""
        
        total = len(responses)
        vulnerable = len([r for r in responses if r.verdict == "vulnerable"])
        safe = len([r for r in responses if r.verdict == "safe"])
        uncertain = len([r for r in responses if r.verdict == "uncertain"])
        errors = len([r for r in responses if r.verdict == "error"])
        
        # Count vulnerabilities by type
        cwe_counts = {}
        severity_counts = {'low': 0, 'medium': 0, 'high': 0, 'critical': 0}
        
        for response in responses:
            for vuln in response.vulnerabilities:
                if vuln.found:
                    cwe_counts[vuln.cwe_id] = cwe_counts.get(vuln.cwe_id, 0) + 1
                    severity_counts[vuln.severity] = severity_counts.get(vuln.severity, 0) + 1
        
        return {
            "total_resolved_patches": total,
            "verdicts": {
                "vulnerable": vulnerable,
                "safe": safe,
                "uncertain": uncertain,
                "error": errors
            },
            "vulnerability_distribution": cwe_counts,
            "severity_distribution": severity_counts,
            "vulnerability_rate": vulnerable / total if total > 0 else 0.0
        }
