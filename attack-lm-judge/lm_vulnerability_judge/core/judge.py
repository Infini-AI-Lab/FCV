"""
Main vulnerability judge system.
"""

import os
import yaml
from pathlib import Path
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..extractors.mini_swe_agent import MiniSweAgentExtractor
from ..extractors.swe_agent import SweAgentExtractor
from ..extractors.openhands import OpenHandsExtractor
from ..judges.qwen_judge import QwenJudge
from ..judges.kimi_judge import KimiJudge
from ..judges.gpt5_mini_judge import GPT5MiniJudge
from ..aggregators.report_generator import ReportGenerator
from ..judges.base import JudgeResponse


class VulnerabilityJudge:
    """Main vulnerability judge system coordinator."""
    
    def __init__(self, config_path: str, args):
        """Initialize with configuration file."""
        self.config = self._load_config(config_path, args)
        self.judge = self._create_judge()
        self.report_generator = ReportGenerator(self.config['output_dir'])
        self.max_workers = getattr(args, 'workers', 30)
    
    def _load_config(self, config_path: str, args) -> Dict:
        """Load configuration from YAML file and override with command line args."""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # Override vulnerabilities from CLI --cwe-type (supports comma/space-separated)
        if hasattr(args, 'cwe_type') and getattr(args, 'cwe_type', None):
            raw = str(getattr(args, 'cwe_type'))
            # split on comma or whitespace
            parts = [p.strip() for chunk in raw.split(',') for p in chunk.split() if p.strip()]
            # normalize to CWE-XXX format
            def norm(x: str) -> str:
                return x if x.startswith('CWE-') else f'CWE-{x}'
            config['vulnerabilities'] = [norm(p) for p in parts]
            print(f"Overriding vulnerabilities with: {config['vulnerabilities']}")
        
        # Override agent paths if provided via command line arguments
        if hasattr(args, 'agent') and args.agent in config.get('agents', {}):
            agent_config = config['agents'][args.agent]
            
            # Override preds_path if provided
            if hasattr(args, 'preds_path') and getattr(args, 'preds_path', None):
                agent_config['preds_path'] = getattr(args, 'preds_path')
                print(f"Overriding preds_path with: {getattr(args, 'preds_path')}")

                if not os.path.exists(agent_config['preds_path']):
                    raise ValueError(f"Preds path does not exist: {agent_config['preds_path']}")
            
            # Override reports_path if provided  
            if hasattr(args, 'reports_path') and getattr(args, 'reports_path', None):
                agent_config['reports_path'] = getattr(args, 'reports_path')
                print(f"Overriding reports_path with: {getattr(args, 'reports_path')}")

                if not os.path.exists(agent_config['reports_path']):
                    raise ValueError(f"Reports path does not exist: {agent_config['reports_path']}")
            
            if hasattr(args, 'agent_path') and getattr(args, 'agent_path', None):
                agent_config['agent_path'] = getattr(args, 'agent_path')
                print(f"Overriding agent_path with: {getattr(args, 'agent_path')}")

                if not os.path.exists(agent_config['agent_path']):
                    raise ValueError(f"Agent path does not exist: {agent_config['agent_path']}")
        
        return config
    
    def _create_judge(self):
        """Create the appropriate judge based on configuration."""
        judge_type = self.config.get('judge_type', 'qwen')
        
        # Get judge-specific configuration, fallback to root config
        if 'judges' in self.config and judge_type in self.config['judges']:
            judge_config = self.config['judges'][judge_type]
        else:
            # Fallback to root config for backwards compatibility
            judge_config = self.config
        
        if judge_type == 'qwen':
            return QwenJudge(judge_config)
        elif judge_type == 'kimi':
            return KimiJudge(judge_config)
        elif judge_type == 'gpt5_mini':
            return GPT5MiniJudge(judge_config)
        else:
            raise ValueError(f"Unknown judge type: {judge_type}. Supported: qwen, kimi, gpt5_mini")
    
    
    def analyze_agent_outputs(self, agent_name: str = 'mini_swe_agent', limit: int = None, failed_pairs: set = None, only_failed: bool = False) -> List[JudgeResponse]:
        """Analyze outputs from a specific agent."""
        
        # Get agent configuration
        agent_config = self.config['agents'][agent_name].copy()
        agent_config['name'] = agent_name
        
        # Add round configuration if available
        if 'rounds' in self.config:
            agent_config.update(self.config['rounds'])
        
        # Select appropriate extractor based on agent type
        if agent_name == 'mini_swe_agent':
            extractor = MiniSweAgentExtractor(agent_config)
        elif agent_name == 'swe_agent':
            extractor = SweAgentExtractor(agent_config)
        elif agent_name == 'openhands':
            extractor = OpenHandsExtractor(agent_config)
        else:
            raise ValueError(f"Unsupported agent type: {agent_name}")
        
        # Extract functionally correct patches
        correct_patches = extractor.get_functionally_correct_patches()
        
        print(f"Found {len(correct_patches)} functionally correct patches from {agent_name}")

        
        # Apply limit if specified
        if limit and limit < len(correct_patches):
            correct_patches = correct_patches[:limit]
            print(f"Limited analysis to {len(correct_patches)} patches")
        
        # Print round statistics if available
        if hasattr(extractor, 'get_round_statistics'):
            round_stats = extractor.get_round_statistics()
            for round_name, stats in round_stats.items():
                print(f"  {round_name}: {stats['functionally_correct']}/{stats['total_patches']} patches correct ({stats['success_rate']:.1%})")
        
        # Assess patches for vulnerabilities with parallel API processing
        responses = self._assess_patches_parallel(correct_patches, failed_pairs, only_failed)
        
        return responses
    
    def _assess_patches_parallel(self, patches: List, failed_pairs: set = None, only_failed: bool = False) -> List[JudgeResponse]:
        """Assess patches with parallel API calls - each API call handles one prompt."""
        
        # Part 1: Build all prompts for all patches
        if only_failed:
            print("Building retry prompts for failed pairs only...")
        else:
            print("Building prompts for all patches...")
        
        all_prompts = []
        patch_to_prompts = {}
        
        for patch in patches:
            prompts = self.judge.build_prompts_for_patch(patch.instance_id, patch.patch_content, failed_pairs, only_failed)
            all_prompts.extend(prompts)
            patch_to_prompts[patch.instance_id] = prompts
        
        print(f"📝 Built {len(all_prompts)} prompts for {len(patches)} patches")
        print(f"📋 Prompts per patch: {len(all_prompts) // len(patches) if patches else 0}")
        
        # Part 2: Call API in parallel for each prompt
        print("Calling API in parallel for each prompt...")
        api_responses = self._call_api_parallel(all_prompts)

        # Part 3: Process results for each patch
        print("Processing results...")
        responses = []
        for i, patch in enumerate(patches, 1):
            print(f"Processing patch {i}/{len(patches)}: {patch.instance_id}")
            
            # Get responses for this patch
            patch_responses = [r for r in api_responses if r['instance_id'] == patch.instance_id]
            
            # Process responses
            response = self.judge.process_api_responses(patch.instance_id, patch_responses)
            responses.append(response)
            
            # Print basic result
            print(f"  Verdict: {response.verdict}")
            if response.vulnerabilities:
                found_vulns = [v for v in response.vulnerabilities if v.found]
                if found_vulns:
                    print(f"  Found vulnerabilities: {[v.cwe_id for v in found_vulns]}")
        
        return responses
    
    def _call_api_with_retry(self, prompt_info: Dict, max_retries: int = 5) -> Dict:
        """Call API with retry mechanism."""
        import time
        
        for attempt in range(max_retries + 1):
            try:
                response_text = self.judge.call_api(prompt_info['prompt'])
                return {
                    'cwe_id': prompt_info['cwe_id'],
                    'instance_id': prompt_info['instance_id'],
                    'response': response_text,
                    'error': None,
                    'attempts': attempt + 1
                }
            except Exception as e:
                print(f"Error: {e}")
                if attempt < max_retries:
                    # Wait before retry (exponential backoff)
                    wait_time = 2 ** attempt
                    print(f"🔄 Retry {attempt + 1}/{max_retries} for {prompt_info['cwe_id']} on {prompt_info['instance_id']} after {wait_time}s delay...")
                    time.sleep(wait_time)
                else:
                    return {
                        'cwe_id': prompt_info['cwe_id'],
                        'instance_id': prompt_info['instance_id'],
                        'response': "",
                        'error': f"Failed after {max_retries + 1} attempts: {str(e)}",
                        'attempts': max_retries + 1
                    }

    def _call_api_parallel(self, prompts: List[Dict]) -> List[Dict]:
        """Call API in parallel for each prompt individually with retry mechanism."""
        responses = []
        total_prompts = len(prompts)
        completed = 0
        retries = 0
        
        print(f"🚀 Starting parallel API calls for {total_prompts} prompts (with retry mechanism) with {self.max_workers} workers...")
        
        # Use ThreadPoolExecutor for parallel API calls
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all API calls with retry wrapper
            future_to_prompt = {
                executor.submit(self._call_api_with_retry, prompt_info): prompt_info
                for prompt_info in prompts
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_prompt):
                prompt_info = future_to_prompt[future]
                completed += 1
                
                try:
                    result = future.result()
                    responses.append(result)
                    
                    if result['error']:
                        status = "❌"
                        retries += 1
                        print(f"⚠️  Failed: {result['cwe_id']} for {result['instance_id']} after {result['attempts']} attempts")
                    else:
                        status = "✅"
                        
                except Exception as e:
                    responses.append({
                        'cwe_id': prompt_info['cwe_id'],
                        'instance_id': prompt_info['instance_id'],
                        'response': "",
                        'error': f"Future exception: {str(e)}",
                        'attempts': 1
                    })
                    status = "❌"
                    retries += 1
                
                # Print progress every 10 completions or at the end
                if completed % 10 == 0 or completed == total_prompts:
                    progress = (completed / total_prompts) * 100
                    print(f"📊 Progress: {completed}/{total_prompts} ({progress:.1f}%) - Last: {status} {prompt_info['cwe_id']} for {prompt_info['instance_id']}")
        
        # Print summary
        successful = len([r for r in responses if not r['error']])
        failed = len([r for r in responses if r['error']])
        print(f"🎉 Completed all {total_prompts} API calls!")
        print(f"📈 Success: {successful} | Failed: {failed} | Retries: {retries}")
        
        return responses
    
    def generate_reports(self, responses: List[JudgeResponse], agent_config: Dict = None) -> Dict[str, str]:
        """Generate reports from assessment responses."""
        
        reports = {}
        
        # Use fixed filename from config if specified
        json_filename = self.config.get('report_filename', None)
        
        # Generate JSON report
        json_path = self.report_generator.generate_json_report(
            responses, 
            filename=json_filename,
            agent_config=agent_config, 
            judge_name=self.config['judge_type']
        )
        reports['json'] = json_path
        print(f"Generated JSON report: {json_path}")
        
        # Generate CSV summary with corresponding filename
        csv_filename = None
        if json_filename:
            csv_filename = json_filename.replace('.json', '.csv')
        
        csv_path = self.report_generator.generate_csv_summary(responses, filename=csv_filename)
        reports['csv'] = csv_path
        print(f"Generated CSV summary: {csv_path}")
        
        # Print summary statistics
        stats = self.report_generator.generate_summary_stats(responses)
        print("\nSummary Statistics:")
        print(f"Total resolved patches analyzed: {stats['total_resolved_patches']}")
        print(f"Vulnerable: {stats['verdicts']['vulnerable']}")
        print(f"Safe: {stats['verdicts']['safe']}")
        print(f"Uncertain: {stats['verdicts']['uncertain']}")
        print(f"Errors: {stats['verdicts']['error']}")
        print(f"Vulnerability rate: {stats['vulnerability_rate']:.2%}")
        
        if stats['vulnerability_distribution']:
            print(f"CWE distribution: {stats['vulnerability_distribution']}")
        
        return reports
    
    def run_full_analysis(self, agent_name: str = 'mini_swe_agent') -> Dict[str, str]:
        """Run complete vulnerability analysis pipeline."""
        
        print(f"Starting vulnerability analysis for {agent_name}...")
        
        # Analyze patches
        responses = self.analyze_agent_outputs(agent_name)
        
        # Get agent configuration for report generation
        agent_config = self.config['agents'][agent_name].copy()
        agent_config['name'] = agent_name
        
        # Generate reports
        reports = self.generate_reports(responses, agent_config=agent_config)
        
        print("Analysis complete!")
        return reports


def main():
    """Main entry point for command line usage."""
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python -m lm_vulnerability_judge <config_path>")
        sys.exit(1)
    
    config_path = sys.argv[1]
    judge = VulnerabilityJudge(config_path)
    judge.run_full_analysis()


if __name__ == "__main__":
    main()
