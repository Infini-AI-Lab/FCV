"""
Extractor for mini-swe-agent format outputs.
"""

import json
import glob
from typing import List, Dict, Any
from pathlib import Path

from .base import BaseExtractor, PatchInfo, FunctionalTestResult


class MiniSweAgentExtractor(BaseExtractor):
    """Extractor for mini-swe-agent prediction and evaluation files."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize extractor with support for separate preds and reports paths."""
        super().__init__(config)
        
        # Support both old data_path format and new separate paths format
        if 'data_path' in config:
            # Old format: everything in one directory
            self.preds_path = Path(config['data_path'])
            self.reports_path = Path(config['data_path'])
        else:
            # New format: separate paths for predictions and reports
            self.preds_path = Path(config.get('preds_path', 'patch/mini-swe-agent'))
            self.reports_path = Path(config.get('reports_path', 'patch/sb-cli-reports'))
    
    def extract_patches(self) -> List[PatchInfo]:
        """
        Extract patches from mini-swe-agent prediction files.
        
        Returns:
            List of PatchInfo objects
        """
        patches = []
        
        # Find all prediction files matching the pattern
        prediction_pattern = self.config.get('prediction_file_pattern', '*-preds.json')
        prediction_files = glob.glob(str(self.preds_path / prediction_pattern))
        
        for pred_file in prediction_files:
            try:
                with open(pred_file, 'r', encoding='utf-8') as f:
                    predictions = json.load(f)
                
                # Extract agent/model name from filename
                file_stem = Path(pred_file).stem
                agent_model = file_stem.replace('-preds', '')
                
                for instance_id, pred_data in predictions.items():
                    patch_content = pred_data.get(self.config.get('patch_field', 'model_patch'), '')
                    
                    if not self.validate_patch(patch_content):
                        continue
                    
                    # Parse file changes and patch size
                    changed_files, patch_size = self._parse_diff(patch_content)
                    
                    # Extract model name from prediction data
                    model_name = self._extract_model_name(pred_data)
                    
                    patch_info = PatchInfo(
                        instance_id=instance_id,
                        agent_name=self.agent_name,
                        model_name=model_name,
                        patch_content=patch_content,
                        is_functionally_correct=False,  # Will be set later
                        test_passed=False,  # Will be set later
                        file_changes=changed_files,
                        patch_size=patch_size,
                        metadata={
                            'prediction_file': pred_file,
                            'agent_model': agent_model,
                            **pred_data
                        }
                    )
                    
                    patches.append(patch_info)
                    
            except Exception as e:
                print(f"Warning: Failed to process prediction file {pred_file}: {e}")
                continue
        
        return patches
    
    def extract_functional_tests(self) -> List[FunctionalTestResult]:
        """
        Extract functional test results from evaluation files.
        
        Returns:
            List of FunctionalTestResult objects
        """
        test_results = []
        
        # Find all evaluation files matching the pattern
        eval_pattern = self.config.get('evaluation_file_pattern', 'swe-bench_verified__test__*-pass*.json')
        eval_files = glob.glob(str(self.reports_path / eval_pattern))
        
        for eval_file in eval_files:
            try:
                with open(eval_file, 'r', encoding='utf-8') as f:
                    eval_data = json.load(f)
                
                # Extract resolved instances (functionally correct)
                resolved_instances = eval_data.get('resolved_ids', [])
                completed_instances = eval_data.get('completed_ids', [])
                
                # Create test results for resolved instances
                for instance_id in resolved_instances:
                    test_result = FunctionalTestResult(
                        instance_id=instance_id,
                        passed=True,
                        test_status='resolved',
                        metadata={
                            'evaluation_file': eval_file,
                            'total_instances': eval_data.get('total_instances', 0),
                            'resolved_count': len(resolved_instances),
                            **eval_data
                        }
                    )
                    test_results.append(test_result)
                
                # Create test results for unresolved instances
                unresolved_instances = [
                    instance_id for instance_id in completed_instances 
                    if instance_id not in resolved_instances
                ]
                
                for instance_id in unresolved_instances:
                    test_result = FunctionalTestResult(
                        instance_id=instance_id,
                        passed=False,
                        test_status='unresolved',
                        metadata={
                            'evaluation_file': eval_file,
                            'total_instances': eval_data.get('total_instances', 0),
                            'resolved_count': len(resolved_instances),
                            **eval_data
                        }
                    )
                    test_results.append(test_result)
                    
            except Exception as e:
                print(f"Warning: Failed to process evaluation file {eval_file}: {e}")
                continue
        
        return test_results
    
    def get_agent_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about the agent's performance.
        
        Returns:
            Dictionary with performance statistics
        """
        patches = self.extract_patches()
        test_results = self.extract_functional_tests()
        correct_patches = self.get_functionally_correct_patches()
        
        # Group by model
        model_stats = {}
        for patch in patches:
            model = patch.model_name
            if model not in model_stats:
                model_stats[model] = {
                    'total_patches': 0,
                    'functionally_correct': 0,
                    'average_patch_size': 0,
                    'files_modified': set()
                }
            
            model_stats[model]['total_patches'] += 1
            model_stats[model]['files_modified'].update(patch.file_changes)
        
        for patch in correct_patches:
            model = patch.model_name
            model_stats[model]['functionally_correct'] += 1
        
        # Calculate averages and convert sets to lists
        for model, stats in model_stats.items():
            if stats['total_patches'] > 0:
                stats['success_rate'] = stats['functionally_correct'] / stats['total_patches']
            else:
                stats['success_rate'] = 0.0
            stats['files_modified'] = list(stats['files_modified'])
        
        return {
            'agent_name': self.agent_name,
            'total_instances': len(test_results),
            'total_patches': len(patches),
            'functionally_correct_patches': len(correct_patches),
            'overall_success_rate': len(correct_patches) / len(patches) if patches else 0.0,
            'model_statistics': model_stats
        }
